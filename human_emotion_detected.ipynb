{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea7edcb1-b311-4fb3-afe9-3de11d96be7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Human_Emotion_Recognition.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile Human_Emotion_Recognition.py\n",
    "import streamlit as st\n",
    "import numpy as np\n",
    "import cv2\n",
    "from deepface import DeepFace \n",
    "from PIL import Image\n",
    "import tempfile\n",
    "\n",
    "\n",
    "st.title(\"Human Emotion Recognition\")\n",
    "\n",
    "options=st.selectbox(\"Choose an Option\",(\"image\",\"video\"))\n",
    "\n",
    "def Analyze_Emotion(image_or_video):\n",
    "    try:\n",
    "        analysis=DeepFace.analyze(image_or_video,actions=['emotion'],enforce_detection=True)\n",
    "        return analysis[0]['emotion']\n",
    "    except ValueError as e:\n",
    "        return None\n",
    "\n",
    "if options=='image':\n",
    "    upload=st.file_uploader('please upload an image', type=['png','jpg','jpeg'])\n",
    "    if upload is not None:\n",
    "        image=Image.open(upload)\n",
    "        image_array=np.array(image)\n",
    "        st.image(image_array,use_container_width=True,channels='RGP')\n",
    "        \n",
    "        emotion_score=Analyze_Emotion(image_array)\n",
    "        if emotion_score:\n",
    "            detected_emotion=max(emotion_score,key=emotion_score.get)\n",
    "            st.header(f\"detected Emotion :{detected_emotion}\") \n",
    "\n",
    "        else:\n",
    "            st.write(\"no face detected\")\n",
    "\n",
    "if options=='video':\n",
    "    upload=st.file_uploader('please upload a video...', type=['mp4','mov','avi'])\n",
    "    if upload is not None:\n",
    "        with tempfile.NamedTemporaryFile(delete=False) as temp_video:\n",
    "            temp_video.write(upload.read())\n",
    "            video_name=temp_video.name\n",
    "        video=cv2.VideoCapture(video_name)\n",
    "\n",
    "        frame_rate=90\n",
    "        frame_count=0\n",
    "        while video.isOpened():\n",
    "            ret,frame=video.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame_count+=1\n",
    "            if frame_count%frame_rate==0:\n",
    "                frame_rgb=cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)\n",
    "                emotion_score=Analyze_Emotion(frame_rgb)\n",
    "                if emotion_score:\n",
    "                    detected_emotion=max(emotion_score,key=emotion_score.get) \n",
    "                    cv2.putText(frame,detected_emotion,(100,60),cv2.FONT_HERSHEY_SIMPLEX,3,(200,20,150),2)\n",
    "                else:\n",
    "                    detected_emotion=\"no face detected\"\n",
    "                    cv2.putText(frame,detected_emotion,(60,60),cv2.FONT_HERSHEY_SIMPLEX,3,(230,20,160),2)\n",
    "                st.image(frame,channels='BGR')\n",
    "        video.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fc451f-b0b7-477e-88d6-22bc9c0648dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (deepface_env)",
   "language": "python",
   "name": "deepface_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
